name: Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    strategy:
      matrix:
        python-version: [3.11, 3.12]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run unit tests
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
        REDIS_URL: redis://localhost:6379
        SECRET_KEY: test-secret-key
        ENVIRONMENT: testing
      run: |
        pytest tests/unit/ -v --cov=app --cov-report=xml --cov-report=html --junitxml=junit.xml

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}
        path: |
          junit.xml
          htmlcov/

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run integration tests
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
        REDIS_URL: redis://localhost:6379
        SECRET_KEY: test-secret-key
        ENVIRONMENT: testing
      run: |
        pytest tests/integration/ -v --junitxml=integration-junit.xml

    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: integration-junit.xml

  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Start application
      run: |
        uvicorn app.main:app --host 0.0.0.0 --port 8080 &
        sleep 10

    - name: Run E2E tests
      run: |
        pytest tests/e2e/ -v --junitxml=e2e-junit.xml

    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-results
        path: e2e-junit.xml

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: e2e-tests
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install locust

    - name: Start application
      run: |
        uvicorn app.main:app --host 0.0.0.0 --port 8080 &
        sleep 10

    - name: Run performance tests
      run: |
        locust -f tests/performance/locustfile.py --host=http://localhost:8080 --users 10 --spawn-rate 2 --run-time 60s --headless --html=performance-report.html

    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-results
        path: performance-report.html

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v3

    - name: Generate test summary
      run: |
        echo "## 🧪 Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Unit tests
        if [[ "${{ needs.unit-tests.result }}" == "success" ]]; then
          echo "✅ **Unit Tests**: Passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **Unit Tests**: Failed" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Integration tests
        if [[ "${{ needs.integration-tests.result }}" == "success" ]]; then
          echo "✅ **Integration Tests**: Passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **Integration Tests**: Failed" >> $GITHUB_STEP_SUMMARY
        fi
        
        # E2E tests
        if [[ "${{ needs.e2e-tests.result }}" == "success" ]]; then
          echo "✅ **End-to-End Tests**: Passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **End-to-End Tests**: Failed" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Performance tests
        if [[ "${{ needs.performance-tests.result }}" == "success" ]]; then
          echo "✅ **Performance Tests**: Passed" >> $GITHUB_STEP_SUMMARY
        elif [[ "${{ needs.performance-tests.result }}" == "skipped" ]]; then
          echo "⏭️ **Performance Tests**: Skipped (not main branch)" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **Performance Tests**: Failed" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 📊 Coverage Report" >> $GITHUB_STEP_SUMMARY
        echo "Check the Codecov report for detailed coverage information." >> $GITHUB_STEP_SUMMARY
